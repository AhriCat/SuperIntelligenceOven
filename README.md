# ğŸ”¥ Superintelligence Oven

**Multi-agent GRPO training framework that doesn't need teacher logits from frontier models.**

Use Claude, DeepSeek, Grok, or any LLM API as a reward committee to train your local model â€” while a hot-swappable local teacher handles the KL anchor with full distributions. No logit access to frontier models required. Model-agnostic: works with text LLMs, diffusion models, motion generators, anything with a trainable forward pass.

---

## The Problem

You want to distill intelligence from frontier models into your local model, but:

- **Frontier APIs don't give you full logit distributions** â€” at best you get top-20 logprobs (DeepSeek, Grok), at worst you get nothing (Claude, GPT)
- **GRPO needs a reward signal, not teacher logits** â€” but most implementations conflate the two
- **Single-teacher distillation bottlenecks** on whatever that one teacher is bad at
- **Your model might not even be a text LM** â€” diffusion, motion, multimodal models need GRPO too

## The Solution

The Superintelligence Oven separates the training signal into independent channels:

| Signal | Source | What it does | Needs logits? |
|---|---|---|---|
| **KL Anchor** | Local teacher (hot-swappable) | Prevents policy collapse | âœ… Full distribution (local) |
| **Reward** | Remote agent swarm (1-5 agents) | Defines "what good looks like" | âŒ Black-box scores only |
| **Semantic Verification** | QwenEmbedVerifier (local) | Embedding-space quality check (text-only) | âŒ Cosine similarity |
| **Frontier Calibration** | DeepSeek/Grok top-k logprobs | Drift detection diagnostic | âš ï¸ Optional, not in loss |
| **Curriculum Steering** | Curriculum agent (API) | Picks what to train on next | âŒ Just text |

The key insight: **logit-level guidance and reward-level guidance are separate channels.** Your local teacher handles the first. Frontier models handle the second. They don't need to be the same model.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local (your GPU)                        â”‚
â”‚                                          â”‚
â”‚  Policy Model â”€â”€generatesâ”€â”€â–º N completions
â”‚       â”‚                                  â”‚
â”‚  Local Teacher â”€â”€providesâ”€â”€â–º KL anchor   â”‚
â”‚  (hot-swappable: small â†” big â†” diffusion)â”‚
â”‚       â”‚                                  â”‚
â”‚  QwenEmbedVerifier â”€â”€â–º semantic score    â”‚
â”‚  (text-only, auto-skipped for non-text)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ completions sent out
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Remote Agent Swarm (async, parallel)    â”‚
â”‚                                          â”‚
â”‚  Agent 1: Critic â”€â”€â”€â”€â”€â”€â”€â”€ Claude Sonnet  â”‚
â”‚  Agent 2: Adversary â”€â”€â”€â”€â”€ DeepSeek R1    â”‚
â”‚  Agent 3: Specialist â”€â”€â”€â”€ Grok 3        â”‚
â”‚  Agent 4: Style â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Claude Haiku  â”‚
â”‚  Agent 5: Curriculum â”€â”€â”€â”€ Claude Sonnet  â”‚
â”‚                                          â”‚
â”‚  Each agent receives ModelDescription    â”‚
â”‚  so it knows what kind of GRPO this is   â”‚
â”‚                                          â”‚
â”‚  â”€â”€â–º weighted composite reward           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ rewards back
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRPO Update                             â”‚
â”‚                                          â”‚
â”‚  advantages = group_normalize(rewards)   â”‚
â”‚  loss = -log_prob(policy) Ã— advantages   â”‚
â”‚       + Î² Ã— KL(policy âˆ¥ local_teacher)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Install

```bash
pip install -e .

# For quantized teachers:
pip install -e ".[quantized]"

# For diffusion teachers:
pip install -e ".[diffusion]"

# Everything:
pip install -e ".[all]"
```

### The `bake()` Function â€” One Call Setup

```python
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer
from superintelligence_oven import bake

tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B", trust_remote_code=True)
policy = AutoModelForCausalLM.from_pretrained("your-model", trust_remote_code=True).to("cuda")

oven = bake(
    model=policy,
    tokenizer=tok,

    # This description is injected into every agent call
    name="my-assistant-7b",
    modality="text",
    domain="general instruction following",
    description="A 7B causal LM for chat. Prioritize accuracy over verbosity.",
    scoring_guidance="Penalize hallucinations heavily. Reward concise answers.",

    prompts=["Explain quantum entanglement.", "Write a merge sort in Python."],
    group_size=8,
    total_steps=5000,
)

oven.run_sync()
```

The `description` and `scoring_guidance` fields become part of every agent's system prompt â€” this is how you tell the reward committee what kind of GRPO they're doing. A motion model gets different scoring criteria than a code model.

### Non-Text Models (Diffusion, Motion)

```python
from superintelligence_oven import bake, TeacherConfig

oven = bake(
    model=my_motion_model,
    name="egirl-motion-v2",
    modality="motion",
    domain="character animation",
    description="VALM motion generator, 72-dim joint trajectories.",
    output_format="trajectory",
    use_verifier=False,  # verifier is text-only â€” skip for motion
    teacher="diffusion_motion",
    teachers=[TeacherConfig(
        name="diffusion_motion",
        kind="diffusion",
        model_name="diffusion-teacher",
        joint_dim=72,
        intent_dim=512,
    )],
    prompts=["wave hello", "dance excitedly"],
)
```

### Swap Agent Models On The Fly

```python
oven.swap_agent("critic",     provider="claude",   model="claude-sonnet-4-20250514")
oven.swap_agent("adversary",  provider="deepseek", model="deepseek-reasoner", logprobs=True)
oven.swap_agent("specialist", provider="grok",     model="grok-3", logprobs=True, top_logprobs=20)
oven.swap_agent("style",      provider="claude",   model="claude-haiku-4-5-20251001")

# Or swap all at once
oven.swap_all_agents(provider="deepseek", model="deepseek-chat")
```

### Hot-Swap Local Teachers

```python
oven.swap_teacher("small_qwen3_4b")     # fast, early training
oven.swap_teacher("big_qwen3_8b_q4")    # deeper, after model absorbs small teacher

# Register new teachers at runtime
from superintelligence_oven import TeacherConfig
oven.oven.teacher_mgr.register_teacher(TeacherConfig(
    name="big_qwen3_32b",
    kind="big",
    model_name="Qwen/Qwen3-32B",
    strategy="hybrid",
))
```

### Wire Into Existing Code

```python
oven.attach_to_model(your_model)
# your_model.teacher = oven's active teacher
# your_model.verifier = oven's QwenEmbedVerifier
```

## Repo Structure

```
superintelligence-oven/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ superintelligence_oven/
â”‚   â”œâ”€â”€ __init__.py              # Public API
â”‚   â”œâ”€â”€ config.py                # All dataclasses, constants, ModelDescription
â”‚   â”œâ”€â”€ oven.py                  # SuperintelligenceOven (main training loop)
â”‚   â”œâ”€â”€ wrapper.py               # OvenWrapper + bake() convenience function
â”‚   â”œâ”€â”€ agents.py                # AgentSwarm (remote reward committee)
â”‚   â”œâ”€â”€ verifier.py              # QwenEmbedVerifier (text-only semantic check)
â”‚   â”œâ”€â”€ calibrator.py            # FrontierCalibrator (diagnostic drift detection)
â”‚   â”œâ”€â”€ prompts.py               # PromptSource (with curriculum injection)
â”‚   â””â”€â”€ teachers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ manager.py           # TeacherManager (hot-swap, auto-escalation)
â”‚       â”œâ”€â”€ small_causal.py      # SmallCausalTeacher (Qwen3-4B)
â”‚       â”œâ”€â”€ big_causal.py        # BigCausalTeacher (quantized/offload/hybrid)
â”‚       â””â”€â”€ diffusion.py         # SmallCausalDiffusionTeacher (motion/trajectory)
â””â”€â”€ examples/
    â”œâ”€â”€ train_text_model.py
    â””â”€â”€ train_motion_model.py
```

## ModelDescription â€” Tell Agents What You're Training

The `ModelDescription` is the bridge between your model and the agent swarm. It gets injected into every agent's system prompt:

```python
from superintelligence_oven import ModelDescription

desc = ModelDescription(
    name="code-wizard-3b",
    modality="text",
    architecture="causal LM",
    domain="code generation",
    description="A 3B model specialized for Python and JavaScript code generation.",
    output_format="text",
    scoring_guidance="Score code for correctness first, then readability, then efficiency.",
    use_verifier=True,
)

# This becomes part of every agent's prompt:
print(desc.to_agent_prompt())
```

## Teacher Types

| Teacher | Kind | VRAM | Speed | Use Case |
|---|---|---|---|---|
| SmallCausalTeacher | `small` | ~2-8 GB | Fast | Text, early training |
| BigCausalTeacher (quantized) | `big` | ~4 GB/8B | Medium | Text, deeper KL signal |
| BigCausalTeacher (offload) | `big` | ~0 GB idle | Slow | VRAM-constrained |
| BigCausalTeacher (hybrid) | `big` | Adaptive | Medium | Balance |
| SmallCausalDiffusionTeacher | `diffusion` | ~1 GB | Fast | Motion/trajectory |

Auto-escalation: when KL between policy and small teacher collapses, the oven swaps to a big teacher automatically.

## Agent Roles

| Agent | Default | Scores? | Purpose |
|---|---|---|---|
| Critic | Claude Sonnet | âœ… | Correctness, coherence, instruction-following |
| Adversary | Claude Sonnet | âœ… | Failure modes, hallucinations, reward hacking |
| Specialist | DeepSeek | âœ… | Domain accuracy, reasoning depth |
| Style | Grok | âœ… | Clarity, tone, formatting |
| Curriculum | Claude Sonnet | âŒ | Steers what to train on next |

Each agent receives the `ModelDescription` so it knows whether it's scoring code, prose, or motion trajectories.

## Supported Providers

| Provider | Scoring | Logprobs | Env Variable |
|---|---|---|---|
| Claude (Anthropic) | âœ… | âŒ | `ANTHROPIC_API_KEY` |
| DeepSeek | âœ… | âœ… top-20 | `DEEPSEEK_API_KEY` |
| Grok (xAI) | âœ… | âœ… top-20 | `XAI_API_KEY` |
| OpenAI | âœ… | âœ… top-20 | `OPENAI_API_KEY` |

## Verifier (Text-Only)

The `QwenEmbedVerifier` uses Qwen3-Embedding-0.6B to compare policy completions against teacher drafts via cosine similarity. It catches garbage (repeated characters, noise, off-topic) before expensive API calls.

**Automatically disabled for non-text models** (motion, diffusion, image) via `use_verifier=False`.

## How GRPO Works Here

1. **Generate** N completions from your policy (you have full logit access locally)
2. **Score** via agent swarm (scalar rewards â€” no logits needed from APIs)
3. **Blend** verifier score (text-only, optional)
4. **Normalize** within group â†’ advantages
5. **Update** policy via clipped gradient, anchored by KL to local teacher
6. **Calibrate** against frontier top-k logprobs (diagnostic, not in loss)
7. **Steer** curriculum agent suggests next prompts

## Requirements

- Python 3.9+
- PyTorch 2.0+
- `transformers`, `accelerate`, `aiohttp`
- At least one API key
- GPU with VRAM for policy + one teacher

## License

MIT

## Contributing

Open issues, PRs, or fork it. The point is everyone should be able to train better models without needing frontier API logits.

---

*Named because it bakes models from all sides simultaneously â€” KL anchor keeps it from burning, agent swarm controls the temperature, GRPO is the convection.*
